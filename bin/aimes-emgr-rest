#!/usr/bin/env python

import os
import sys
import time
import glob
import json
import errno
import bottle
import threading
from   datetime import datetime

import radical.utils as ru

import aimes.emgr


'''
We consider a emgr session to represent one more-or-less coherent workload which
is submitted as one or more streams of tasks.  We chunk those streams into
arbitrary chunks which we call 'emgr-workloads', and hand it over to the
aimes-emgr layer for execution.  The chunking is determindes by timeout, but
eventually should be replaced by an explicit signal. 

We will need to maintain a mapping from tasks to workloads, so that we can
report state changes to the callee.  For that we use the CU 'name' property.  We
also ask the emgr to register a state callback for us, because otherwise we'd
never lean about unit state progressions.

The API is basically:

  emgr_sid = create_emgr_session()
  emgr_tid = submit_emgr_task(emgr_sid, description)
  state    = check_state(emgr_sid, emgr_tid)

On disk, we basically have this layout:

data/
|
+ -- aimes/
|    |
|    +-- emgr_sid/
|        |
|        + -- session_ids.txt # list of RP sessions IDs for each emgr session
|        + -- <PR session ID 1>/
|        + -- ...
|        + -- <PR session ID n>/
|             |
|             + -- cfg.json
|             + -- work.json
|
+ -- emgr/
     |
     +-- emgr_sid/
         |
         + -- timestamp   # time of last workload execution
         + -- workload_01/
         |    |
         |    +-- units_01.json
         |    +-- units_02.json
         |
         + -- workload_02/
         |    |
         |    +-- units_03.json
         |
         | -- workload --> workload_02/
              # this symbolic link is always pointing to the workload which is
              # currently filled for execution.  When the times execution runs
              # this workload, it will reroute the link to a new workload.
              # this is why the session lock exists.

In order to map task descriptions to units IDs, we set the CU name to the task
sequence number (which is used as task ID).  When getting an RP callback for
a CU state change, we can inspect the CU name and map it back to the task ID.
'''

# =============================================================================
# 
# configuration, bootstrap and utilities
#
# -----------------------------------------------------------------------------
def usage(msg=None, noexit=False):

    if msg:
        print "\nError: %s" % msg

    print """
    usage   : %s <conf_file>.json

    """ % (sys.argv[0])

    if msg:
        sys.exit(1)

    if not noexit:
        sys.exit(0)

# ------------------------------------------------------------------------------
#
# read configuration

if len(sys.argv) <= 1:
    usage("insufficient arguments -- need session ID")

if len(sys.argv) > 2:
    usage("too many arguments -- no more than 1")

config = ru.read_json(sys.argv[1])

# Execution
timeout    = 5   # run workload after 10 seconds of idleness
fake_ttc   = 30  # seconds until a faked task execution is declared done

# logging
root       = config["path"]
data_swift = "%s/swift" % config["path"]
data_aimes = "%s/aimes" % config["path"]

os.system('mkdir -p %s' % data_swift)
os.system('mkdir -p %s' % data_aimes)

# session registry
sessions = dict()
SESSION_TIMEOUT = 600 # seconds idle before close

# we lock timestamps and sessions
ts_lock      = threading.RLock()
session_lock = threading.RLock()


# ------------------------------------------------------------------------------
#
def mkdir(path):
    try:
        os.makedirs(path)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise


# ------------------------------------------------------------------------------
#
def unlink(path, name):
    os.remove("%s/%s" % (path, name))


# ------------------------------------------------------------------------------
#
def link(path, src, tgt):
    print "cd %s && ln -s %s %s" % (path, src, tgt)
    os.system("cd %s && ln -s %s %s" % (path, src, tgt))


# ------------------------------------------------------------------------------
#
# update timestamps for workloads.  See documentation of _workload_watcher() to
# see why
#
def _timestamp(emgr_sid):

    now = time.time()
    with ts_lock:
        os.system('echo "%s" > %s/%s/timestamp' % (now, data_swift, emgr_sid))


# ------------------------------------------------------------------------------
#
def _get_timestamp(emgr_sid):

    with ts_lock:
        with open("%s/%s/timestamp" % (data_swift, emgr_sid), "r") as f:
            return float(f.read().strip())


# ------------------------------------------------------------------------------
#
# get state notifications
#
def _state_cb(unit, state):

    unit_d = unit.as_dict()

    # we dig the emgr_sid and emgr_tid out of the unit name
    if not unit_d['name'] or ':' not in unit_d['name']:
        print "state_cb: invalid units name: %s" % unit_d['name']
        return True

    emgr_sid, emgr_tid = unit_d['name'].split(':', 1)

    with session_lock:

        if emgr_sid not in sessions:
            print "state_cb: invalid session id: %s" % emgr_sid
            return True

        if emgr_tid not in sessions[emgr_sid]['tasks']:
            print "state_cb: invalid task id: %s" % emgr_tid
            return True

        print 'state_cb: updating task state: %s:%s : %s' % (emgr_sid, emgr_tid, state)
        sessions[emgr_sid]['tasks'][emgr_tid]['state'] = state
        return True


# ==============================================================================
#
# session management
#
# ------------------------------------------------------------------------------
# create a new swift workload
@bottle.route('/emgr/sessions/', method='PUT')
def emgr_session_create():

    emgr_sid = aimes.emgr.create_session()

    if os.path.exists('%s/%s/' % (data_swift, emgr_sid)):
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "error"   : "swift session '%s' exists" % emgr_sid}

    with session_lock:
        sessions[emgr_sid] = dict()
        sessions[emgr_sid]['timer'] = time.time()
        sessions[emgr_sid]['tasks'] = dict()

        # active workload executions
        sessions[emgr_sid]['threads'] = list()

        # counter for generating task IDs
        sessions[emgr_sid]['tcnt'] = 0

        # counter for generating workload IDs
        sessions[emgr_sid]['wlcnt'] = 0

        sessions[emgr_sid]['cwd'] = "%s/%s" % (data_swift, emgr_sid)
        sessions[emgr_sid]['wld'] = "%s/%s/workload" % (data_swift, emgr_sid)

        # create the first workload dir, and link 'workload' to it
        cwd     = sessions[emgr_sid]['cwd']
        wl_name = 'workload_%d' % sessions[emgr_sid]['wlcnt']

        mkdir('%s/%s/%s' % (data_swift, emgr_sid, wl_name))
        link(cwd, wl_name, 'workload')

    return {"success" : True,
            "emgr_sid": emgr_sid,
            "result"  : "swift session '%s' has been created" % emgr_sid}


# ------------------------------------------------------------------------------
# list all sessions
@bottle.route('/emgr/sessions/', method='GET')
def emgr_session_list():

    with session_lock:
        return {"success" : True  ,
                "paths"   : sessions.keys() }


# ------------------------------------------------------------------------------
# list currently defined units in the session
@bottle.route('/emgr/sessions/<emgr_sid>', method='GET')
def emgr_session_list(emgr_sid=None):

    if not emgr_sid:
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "error"   : "missing swift session id"}

    # look for in data/emgr/<emgr_sid>/
    if emgr_sid not in sessions:
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "error"   : "swift session id %s does not exist" % emgr_sid}

    wld = sessions[emgr_sid]['wld']
    if not os.path.exists(wld):
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "error"   : "swift session %s is inconsistent" % emgr_sid}

    ret = dict()
    ret['tasks'] = dict()
    for t in sessions[emgr_sid]['tasks']:
        ret['tasks'][t] = dict()
        ret['tasks'][t]['state'] = sessions[emgr_sid]['tasks'][t]['state']

    # this action keeps the session alive
    sessions[emgr_sid]['timer'] = time.time()

    bottle.response.content_type = 'application/json'
    return {"success" : True,
            "emgr_sid": emgr_sid,
            "result"  : ret}

# ------------------------------------------------------------------------------
# delete session
@bottle.route('/emgr/sessions/<emgr_sid>', method='DELETE')
def emgr_session_delete(emgr_sid=None):

    # NOTE: this will not cancel any tasks

    if not emgr_sid:
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "error"   : "missing swift session id"}

    # look for in data/emgr/<emgr_sid>/
    if emgr_sid not in sessions:
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "error"   : "swift session id %s does not exist" % emgr_sid}

    wld = sessions[emgr_sid]['wld']
    if not os.path.exists(wld):
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "error"   : "swift session %s is inconsistent" % emgr_sid}

    with session_lock:
        aimes.emgr.cancel_overlay(emgr_sid)
        del(sessions[emgr_sid])
        os.system('rm -rf %s/%s' % (data_swift, emgr_sid))

    return {"success" : True,
            "emgr_sid": emgr_sid,
            "result"  : "workload %s deleted" % emgr_sid}



# ==============================================================================
#
# workload management
#
# ------------------------------------------------------------------------------
#
# run a (partial) workload
#
def _run_workload(emgr_sid):

    try:
        with session_lock:
            cwd = sessions[emgr_sid]['cwd']
            wld = sessions[emgr_sid]['wld']
            if not os.path.exists(wld):
                raise ValueError("swift session id %s is inconsistent" % emgr_sid)

            # we only need to do anything if there are tasks to be executed
            task_files = glob.glob("%s/*.json" % wld)
            if not task_files:
                print "no tasks to execute for session %s (refreshing timestamp)" % emgr_sid
                _timestamp(emgr_sid)
                return None

            # avoid rerunning units on the next timeout, by creating a new
            # workload dir for new units, and re-linking the 'workload' link to
            # that one
            old_wl_name = 'workload_%d' % sessions[emgr_sid]['wlcnt']
            sessions[emgr_sid]['wlcnt'] += 1
            new_wl_name = 'workload_%d' % sessions[emgr_sid]['wlcnt']

            mkdir('%s/%s/%s' % (data_swift, emgr_sid, new_wl_name))
            unlink(cwd, 'workload')
            link(cwd, new_wl_name, 'workload')

            # at this point we can actually release the session lock -- the new
            # wld pointee is in place and can be used.

        # collect all tasks into a emgr_workload, as compute unit
        # descriptions
        sw = dict()
        sw['cuds'] = list()

        paths = glob.glob("%s/%s/*" % (cwd, old_wl_name))
        for f in paths:
            sw['cuds'].append(ru.read_json(f))

        cfg = config

        cfg['scales'] = [len(sw['cuds'])]
        cfg['rerun_attempts'] = 1

        # we assume a single run. This makes scales, bindings, time_distribs,
        # iterations, and cores unecessary lists. We are going to assume that
        # each list as a single value otherwise we trow an error.
        if  len(cfg['scales'])        > 1 or \
            len(cfg['bindings'])      > 1 or \
            len(cfg['time_distribs']) > 1 or \
            len(cfg['iterations'])    > 1 or \
            len(cfg['cores'])         > 1 :
            print "Multiple scales, bindings, time distributions, iterations, \
                   and number of cores not supported in this execution mode"
            pass
        else:
            aimes.emgr.create_run_environment(emgr_sid, 
                                              cfg,
                                              [cfg['scales'][0],
                                               cfg['bindings'][0],
                                               cfg['time_distribs'][0],
                                               cfg['iterations'][0],
                                               cfg['rerun_attempts'],
                                               cfg['cores'][0]
                                              ], 1, 1)

        # we are not really interested in the workload execution result -- we
        # watch the individual units.  So we execute the workload in the
        # background in aseparate thread.
        def _wl_executor(emgr_sid, cfg, sw):
            # run the workload, and also pass the state callback so that we get
            # state notifications
            rp_sid = aimes.emgr.create_overlay(emgr_sid, cfg, sw)
            rp_sid = aimes.emgr.execute_workload(emgr_sid, cfg, sw, _state_cb)

            session_dir = '%s/%s' % (data_aimes, emgr_sid)
            mkdir("%s/%s/" % (session_dir, rp_sid))
            ru.write_json(cfg, "%s/%s/cfg.json"  % (session_dir, rp_sid))
            ru.write_json(sw,  "%s/%s/work.json" % (session_dir, rp_sid))
            with open("%s/session_ids.txt"       % (session_dir), "a+") as f:
                f.write("%s\n" % rp_sid)

        wl_thread = threading.Thread(target=_wl_executor, args=[emgr_sid, cfg, sw])
        wl_thread.start()
        sessions[emgr_sid]['threads'].append(wl_thread)

        # remove the timestamp -- no need to watch this workload until new units
        # arrive
        try:
            os.unlink("%s/timestamp" % cwd)
        except:
            pass

    except Exception as e:
        import logging
        logging.exception('oops')
        print "ERROR: workload execution failed: %s" % e
        raise


# ------------------------------------------------------------------------------
# add a task to a swift workload
@bottle.route('/emgr/sessions/<emgr_sid>', method='PUT')
def emgr_task_submit(emgr_sid=None):

    if not emgr_sid:
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "emgr_tid": None,
                "error"   : "missing session id"}

    # look for in data/emgr/<emgr_sid>/
    if emgr_sid not in sessions:
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "emgr_tid": None,
                "error"   : "swift session id %s does not exist" % emgr_sid}

    wld = sessions[emgr_sid]['wld']
    if not os.path.exists(wld):
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "emgr_tid": None,
                "error"   : "swift session %s is inconsistent" % emgr_sid}

    td = json.loads(bottle.request.forms.get("td"))

    if not td:
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "emgr_tid": None,
                "error"   : "missing task description"}

    with session_lock:
        emgr_tid = 'task.%06d' % sessions[emgr_sid]['tcnt']
        sessions[emgr_sid]['tcnt'] += 1

        # we set the swift task id as CU name, so that we can identify the ytask
        # later, when getting RP state updates
        td['name'] = "%s:%s" % (emgr_sid, emgr_tid)

        fname = '%s/%s.json' % (wld, emgr_tid)
        ru.write_json(td, fname)
        sessions[emgr_sid]['tasks'][emgr_tid] = dict()
        sessions[emgr_sid]['tasks'][emgr_tid]['td']      = td
        sessions[emgr_sid]['tasks'][emgr_tid]['state']   = 'New'
        sessions[emgr_sid]['tasks'][emgr_tid]['created'] = time.time()

        _timestamp(emgr_sid)

    # this action keeps the session alive
    sessions[emgr_sid]['timer'] = time.time()

    bottle.response.content_type = 'application/json'
    return {"success" : True,
            "emgr_sid": emgr_sid,
            "emgr_tid": emgr_tid,
            "result"  : "task '%s' created for swift session %s" % (emgr_tid, emgr_sid)}


# ------------------------------------------------------------------------------
# inspect a single task
@bottle.route('/emgr/sessions/<emgr_sid>/<emgr_tid>', method='GET')
def emgr_task_state(emgr_sid=None, emgr_tid=None):

    if not emgr_sid:
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "emgr_tid": None,
                "error"   : "missing swift session id"}

    if not emgr_tid:
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "emgr_tid": emgr_tid,
                "error"   : "missing swift task id"}

    if emgr_sid not in sessions:
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "error"   : "swift session id %s does not exist" % emgr_sid}

    if emgr_tid not in sessions[emgr_sid]['tasks']:
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "emgr_sid": emgr_tid,
                "error"   : "swift task id %s does not exist in session %s" % \
                    (emgr_tid, emgr_sid)}

    with session_lock:
        ret = sessions[emgr_sid]['tasks'][emgr_tid]

    # this action keeps the session alive
    sessions[emgr_sid]['timer'] = time.time()

    bottle.response.content_type = 'application/json'
    return {"success" : True,
            "emgr_sid": emgr_sid,
            "emgr_tid": emgr_tid,
            "result"  : ret}


# ------------------------------------------------------------------------------
# cancel a single task
@bottle.route('/emgr/sessions/<emgr_sid>/<emgr_tid>', method='DELETE')
def emgr_task_cancel(emgr_sid=None, emgr_tid=None):

    # note that this currently only deletes all knowledge about a task, but does
    # not actually attempt to cancel the task instance
    if not emgr_sid:
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "emgr_tid": None,
                "error"   : "missing swift session id"}

    if not emgr_tid:
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "emgr_tid": emgr_tid,
                "error"   : "missing swift task id"}

    if emgr_sid not in sessions:
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "error"   : "swift session id %s does not exist" % emgr_sid}

    if emgr_tid not in sessions[emgr_sid]['tasks']:
        return {"success" : False,
                "emgr_sid": emgr_sid,
                "emgr_sid": emgr_tid,
                "error"   : "swift task id %s does not exist in session %s" % \
                    (emgr_tid, emgr_sid)}

    with session_lock:
        del(sessions[emgr_sid]['tasks'][emgr_tid])

    # this action keeps the session alive
    sessions[emgr_sid]['timer'] = time.time()

    bottle.response.content_type = 'application/json'
    return {"success" : True,
            "emgr_sid": emgr_sid,
            "emgr_tid": emgr_tid,
            "result"  : "Task %s in session %s deleted" % (emgr_tid, emgr_sid)}


# ------------------------------------------------------------------------------
#
# Swift will never know if more tasks get submitted, so it cannot sensibly call
# 'emgr_workload_run'.  We thus run a separate thread which watches all
# sessions, and if a workload did not see any new units added for N seconds, it
# will call run on it.
#
# FIXME: any error on workload execution will not be visible to the user of the
#        service.  We should log errors to a file and add a 'workload_status'
#        method.
#
def _workload_watcher(terminate):

    print "timed workload watcher started"

    while not terminate.is_set():

        for emgr_sid in sessions:

            # print "timed workload check for %s" % emgr_sid
            ts_file = "%s/%s/timestamp" % (data_swift, emgr_sid)

            if not os.path.exists(ts_file):
                # workload does not need any watching
                continue

            # check if enough time passed -- if not, nothing to do
            last_action = _get_timestamp(emgr_sid)
            if (time.time() - last_action) < timeout:
                print "timed workload check for %s skipped (%.2f < %.2f)" \
                        % (emgr_sid, (time.time() - last_action), timeout)
                continue

            # this action keeps the session alive
            sessions[emgr_sid]['timer'] = time.time()

            # we should run this workload
            try:
                print "timed workload run for %s" % emgr_sid
                _run_workload(emgr_sid)
            except Exception as e:
                print "error on timed workload run for %s: %s" % (emgr_sid, e)

        # avoid busy loop
        time.sleep(1)

# THE DEMON STARTS HERE! start the workload watcher
terminate = threading.Event()
watcher   = threading.Thread(target=_workload_watcher, args=[terminate])
watcher.start()

# ------------------------------------------------------------------------------
#
# Swift will also never terminate sessions, so we start another watcher which
# will cycle through all sessions, and if it finds one which is idle for longer
# than 10 mintes, will close it.
def _session_watcher(terminate):

    print "timed session watcher started"

    while not terminate.is_set():

        to_cancel = list()
        with session_lock:

            for esid in sessions[:]:

                if time.time() - sessions[esid]['timer'] > SESSION_TIMEOUT:
                    to_cancel.append(esid)
                    del(sessions[esid])
                
        for esid in to_cancel:
            aimes.emgr.cancel_overlay(esid)

        # avoid busy loop
        time.sleep(1)

swatcher = threading.Thread(target=_session_watcher, args=[terminate])
swatcher.start()


# ==============================================================================
#
# main
#
# ------------------------------------------------------------------------------

# start bottle main loop.  
# this will block as long as the service is alive.
try:
    bottle.run(host='localhost', port=8080, debug=True)

finally:
    # the service terminated.  We don't have all state on disk, so will not be
    # able to pick up any sessions, workloads or tasks which are currently
    # executed -- so we can just as well clean the slate. but first stop the
    # watcher so that no new workloads get executed...
    terminate.set()
    watcher.join()
    swatcher.join()

    # kill any session which has not been terminated by the swatcher
    for esid in sessions:
        aimes.emgr.cancel_overlay(esid)
    
    # we keep the data dir for debugging...
    # os.system('rm -r %s' % data_swift)
    
# ------------------------------------------------------------------------------

